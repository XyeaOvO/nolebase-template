### 如何度量和表示信息？🤔 -> 到底啥是“信息量”？

想象一下，你在等一个重要的考试成绩。

*   **情况一：** 你是个学霸，平时门门功课都接近满分。老师告诉你：“你这次又考了高分！”
    *   你听到这个消息，会很惊讶吗？可能不太会，因为这差不多是**意料之中**的事。这件事发生的**概率很高**。所以，这条消息带给你的“新信息”其实不多，**信息量比较小**。

*   **情况二：** 你平时成绩一般，勉强及格。老师突然告诉你：“恭喜你！这次考了全班第一！”
    *   你听到这个消息，是不是会惊掉下巴？“哇！真的假的？！” 这件事发生的**概率很低**，完全**出乎意料**。所以，这条消息带给你的“新信息”就非常多，**信息量巨大**！💥

**总结一下：**
一件事情发生的**可能性越小 (概率低)**，当它真的发生了，带给你的**冲击越大，信息量就越大**。
反之，一件事情发生的**可能性越大 (概率高)**，它发生时带给你的**信息量就越小**。

这就是“信息量”的核心思想：**不确定性越大，信息量越大。**

---

#### 单个事件的信息量 (自信息) 😯 -> 具体怎么算一个消息的“惊讶程度”？

科学家们想给这种“惊讶程度”或“信息量”定个标准。他们发现用数学上的“对数”(log) 来表示很合适。

公式是： $I(x) = -\log_a P(x)$

*   $P(x)$ 就是事件 $x$ 发生的概率 (比如你考第一的概率)。
*   $\log_a$ 就是取对数。
    *   如果 $a=2$，单位就是我们常说的**比特 (bit)** 👍。为啥用2呢？因为计算机世界是二进制的，很多信息最终都可以归结为“是”或“否”的两种状态，就像抛硬币一样。
        *   抛一枚均匀的硬币，正面朝上的概率是 $P(\text{正面}) = 0.5$。
        *   它包含的信息量是 $I(\text{正面}) = -\log_2 (0.5) = -\log_2 (1/2) = -(-1) = 1$ 比特。
        *   意思是，知道硬币是正面还是反面，你需要1比特的信息来确定。
*   那个负号 `-` 是因为概率 $P(x)$ 是0到1之间的数，它的对数是负的，加个负号就变成正的了，我们习惯信息量是正数。

**简单说：**
一个事件的概率 $P(x)$ 越小，$\log_2 P(x)$ 的绝对值就越大，所以 $I(x)$ 就越大。这跟我们刚才说的“越不可能发生的事，信息量越大”完全一致！

**多个独立事件：**
如果你连续收到好几个**互不相干**的“惊喜”消息，总的“惊喜程度”就是每个消息“惊喜程度”的叠加。比如，你考了第一 (惊喜A)，你买的彩票中了奖 (惊喜B)，这两个惊喜加起来让你更惊喜！数学上，$\log(P_A \times P_B) = \log P_A + \log P_B$，正好能体现这种叠加。

---

#### 多个事件的平均信息量 (信息熵) 📊 -> 一堆消息，平均每个消息有多“惊讶”？

现在我们不只看单个孤立的事件了，而是看一个**信息源**，比如一部电报机、一个电视台，或者你关注的一个八卦博主。他们会不断地发出各种消息。

*   有的消息很常见，信息量小 (比如天气预报说明天晴，如果当地天天晴)。
*   有的消息很罕见，信息量大 (比如天气预报说明天撒哈拉沙漠下雪)。

**信息熵 (Entropy)，说的就是这个信息源发出的所有消息，平均下来，每条消息带有多大的信息量（或者说，平均有多大的“不确定性”或“惊喜度”）。**

🧠 **知识回顾**: 概率论里我们常算“期望值”，就是各种可能值的加权平均。信息熵也是一种期望值。

公式是： $H(X) = -\sum_{i=1}^M P(x_i) \log_2 P(x_i)$

*   $X$ 代表这个信息源。
*   $x_i$ 是信息源可能发出的第 $i$ 种消息 (符号)。
*   $P(x_i)$ 是第 $i$ 种消息出现的概率。
*   $-\log_2 P(x_i)$ 是第 $i$ 种消息自身的“惊讶程度”(自信息量)。
*   $\sum$ 就是把所有可能消息的“(出现概率) × (自身的惊讶程度)”加起来。

**打个比方：**
假设一个“中文打字机”只能打出两个字：“的”(出现概率90%) 和 “龙”(出现概率10%)。
*   “的”的自信息量：$I(\text{的}) = -\log_2(0.9) \approx 0.15$ 比特 (不怎么惊讶)
*   “龙”的自信息量：$I(\text{龙}) = -\log_2(0.1) \approx 3.32$ 比特 (比较惊讶)

这个打字机的**信息熵 (平均惊讶程度)** 就是：
$H = P(\text{的}) \times I(\text{的}) + P(\text{龙}) \times I(\text{龙})$
$H = 0.9 \times 0.15 + 0.1 \times 3.32 \approx 0.135 + 0.332 = 0.467$ 比特/字。
意思是，这个打字机平均每打一个字，大约提供了 0.467 比特的信息。

**信息熵有什么用？**
它告诉我们一个信息源的**内在不确定性有多大**。
*   如果一个信息源非常**单调** (比如一个坏掉的复读机只会说“你好你好你好”)，那么它的信息熵就很低 (接近0)，因为下一个字是什么几乎是确定的，没什么新信息。
*   如果一个信息源非常**丰富多变且难以预测** (比如莎士比亚全集)，那么它的信息熵就很高。

**特殊情况 (最“乱”的情况)：**
如果一个信息源有 $M$ 种可能的符号，而且每种符号出现的概率都**完全一样** (都是 $1/M$)，那么这个信息源就是最不可预测的，它的信息熵达到最大值，等于 $\log_2 M$。
比如，一个有 $M=2^k$ 个等概率符号的信源，它的信息熵就是 $k$ 比特/符号。这意思是，平均来说，你需要 $k$ 个比特才能表示这个信源发出的一个符号。这在数据压缩里很有用！
